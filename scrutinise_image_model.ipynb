{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b830a3e4-9cdf-478e-9e40-218457a49f11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/studio-lab-user/swoop-machine-learning-v2\n"
     ]
    }
   ],
   "source": [
    "%cd swoop-machine-learning-v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20dcc849-ef7f-441c-906d-b6792db1cc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ai_functions\n",
    "# cannot import boto3, sklearn, utils definitions\n",
    "import ai_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5603e1a3-5d0e-4d61-b919-1caacf710b20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ENVIRONMENT = 'testing'\n",
    "AI_BUCKET = \"snapdragon-{}-ai\".format(ENVIRONMENT) #definitions.py\n",
    "# s3_client = boto3.client(\"s3\")\n",
    "# s3_bucket = boto3.resource(\"s3\").Bucket(AI_BUCKET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ed1c0f9-5c78-4247-b20b-4cb9f3a87fa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inside keras?\n",
    "MODEL_WEIGHTS_LOCATION = \".keras/VGG16_weights/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b5e18ccb-6d71-4734-a704-4f591fd89f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When calling ImageModel, need image_json, data, product_id, result_key, can_test, logger arguments needed\n",
    "class ImageModel:\n",
    "    def __init__(self, image_json, data, product_id, result_key, can_test, logger):\n",
    "        self.image_json = image_json\n",
    "        self.product_id = product_id\n",
    "        self.result_key = result_key\n",
    "        self.prediction_data = data\n",
    "        self.test_data = data[data[\"image\"].notna()]\n",
    "        self.can_test = can_test,\n",
    "        self.hash_lists = {} # fill image hash lists\n",
    "        self.tmp_dir = None\n",
    "        self.dir_lists = {}\n",
    "        self.model = None\n",
    "        # from keras.preprocessing.image import ImageDataGenerator\n",
    "        # constants from image_model_constants.py. It is in utils folder.\n",
    "        self.train_datagen = ImageDataGenerator(\n",
    "            rescale=1. / 255,  # rescales pixel values\n",
    "            rotation_range=constants.ROTATION_RANGE,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "            zoom_range=constants.ZOOM_RANGE, # Randomly zoom image\n",
    "            width_shift_range=constants.WIDTH_SHIFT_RANGE,  # randomly shift images horizontally (fraction of total width)\n",
    "            height_shift_range=constants.HEIGHT_SHIFT_RANGE,  # randomly shift images vertically (fraction of total height)\n",
    "            horizontal_flip=constants.HORIZONTAL_FLIP,  # randomly flip images\n",
    "            vertical_flip=constants.VERTICAL_FLIP,  # randomly flip images\n",
    "        )\n",
    "        self.val_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "        self.test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "        self.predict_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "        self.generators = {}\n",
    "        self.model_details = {}\n",
    "        self.test_probabilities = None\n",
    "        self.test_results = pd.DataFrame()\n",
    "        self.roc_auc = None\n",
    "        self.prediction_probabilities = None\n",
    "        self.prediction_results = pd.DataFrame(columns=[\"image_score\", \"image\"])\n",
    "        self.prediction_generator_filenames = None\n",
    "        self.logger = logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "946201e1-8f68-46ed-8e75-5ee68681878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_image_hash_lists. self.hash_lists is defined empty {}\n",
    "# wonder what's the output of image_json[\"infringing_hashes\"]\n",
    "def set_image_hash_lists(self):\n",
    "    self.hash_lists[\"infringing_hashes\"] = self.image_json[\"infringing_hashes\"]\n",
    "    self.hash_lists[\"innocent_hashes\"] = self.image_json[\"innocent_hashes\"]\n",
    "    self.hash_lists[\"irrelevant_hashes\"] = self.image_json[\"irrelevant_hashes\"]\n",
    "    self.logger.info(\"Image training hash lists compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c07e1a82-7f03-4196-a9c8-ee8af33249f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_temporary_directory_structure(self):\n",
    "    self.tmp_dir = tempfile.mkdtemp()\n",
    "\n",
    "    os.mkdir(self.tmp_dir + \"/train_dir\")\n",
    "    os.mkdir(self.tmp_dir + \"/train_dir/infringing\")\n",
    "    os.mkdir(self.tmp_dir + \"/train_dir/innocent\")\n",
    "    os.mkdir(self.tmp_dir + \"/train_dir/irrelevant\")\n",
    "\n",
    "    os.mkdir(self.tmp_dir + \"/val_dir\")\n",
    "    os.mkdir(self.tmp_dir + \"/val_dir/infringing\")\n",
    "    os.mkdir(self.tmp_dir + \"/val_dir/innocent\")\n",
    "    os.mkdir(self.tmp_dir + \"/val_dir/irrelevant\")\n",
    "\n",
    "    os.mkdir(self.tmp_dir + \"/test_dir\")\n",
    "    os.mkdir(self.tmp_dir + \"/test_dir/infringing\")\n",
    "    os.mkdir(self.tmp_dir + \"/test_dir/innocent\")\n",
    "    os.mkdir(self.tmp_dir + \"/test_dir/irrelevant\")\n",
    "\n",
    "    os.mkdir(self.tmp_dir + \"/keras_model\")\n",
    "    os.mkdir(self.tmp_dir + \"/keras_model/variables\")\n",
    "    os.mkdir(self.tmp_dir + \"/keras_model/assets\")\n",
    "\n",
    "    self.logger.info(\"Training temporary directories created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "978f47eb-b8c4-43ce-a95f-fffb3f207499",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_image_lists(self):\n",
    "    random.shuffle(self.hash_lists[\"infringing_hashes\"])\n",
    "    random.shuffle(self.hash_lists[\"innocent_hashes\"])\n",
    "    random.shuffle(self.hash_lists[\"irrelevant_hashes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44b5f2af-53c7-458e-bbb5-835a8e57fd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class_label, required_image_count, excluded_images should be included as arguments\n",
    "# self.test_data = data[data[\"image\"].notna()] == class_label. What is [\"class\"] column? [0,1,2] [infringing, innocent, irrlevant]\n",
    "# self.prediction_data = data\n",
    "\n",
    "def get_lists_for_test_directory(self, class_label, required_image_count, excluded_images):\n",
    "    dataframe = self.test_data[self.test_data[\"class\"] == class_label]\n",
    "    # except ['iamge'] column, isin(exclude_images) & \"empty\"\n",
    "    dataframe = dataframe[~dataframe[\"image\"].isin(excluded_images)]\n",
    "    dataframe = dataframe[~dataframe[\"image\"].isin([\"empty\"])]\n",
    "    \n",
    "    # randomly sample with minimum number of required_image_count\n",
    "    # return dataframe['image'] as a list.\n",
    "    dataframe = dataframe.sample(n=min(required_image_count, len(dataframe)))\n",
    "    return dataframe[\"image\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "177cd13a-c1dd-4dff-a3fe-ba3d9b39ba7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in dir_lists {}, adding train, val infringing, innocent, irrelevant hashes.\n",
    "# self.dir_lists is an empty {}. Why not empty [] but {}? Each dictionary A,B,C,D can contain 8-12 items (lists). This is neat and convenient for the furue use.\n",
    "# hash_lists[\"infringing_hashes\"][:int(len*0.75)]. Why :int? because len*0.75 can be float. Make it int and slicing.\n",
    "def get_lists_for_each_directory(self):\n",
    "    # TRAIN_VAL_SPLIT = 0.75 in constants.py\n",
    "        self.dir_lists[\"train_infringing_hashes\"] = self.hash_lists[\"infringing_hashes\"][:int(\n",
    "            len(self.hash_lists[\"infringing_hashes\"]) * constants.TRAIN_VAL_SPLIT)]\n",
    "        self.dir_lists[\"val_infringing_hashes\"] = self.hash_lists[\"infringing_hashes\"][int(\n",
    "            len(self.hash_lists[\"infringing_hashes\"]) * constants.TRAIN_VAL_SPLIT):]\n",
    "\n",
    "        self.dir_lists[\"train_innocent_hashes\"] = self.hash_lists[\"innocent_hashes\"][:int(\n",
    "            len(self.hash_lists[\"innocent_hashes\"]) * constants.TRAIN_VAL_SPLIT)]\n",
    "        self.dir_lists[\"val_innocent_hashes\"] = self.hash_lists[\"innocent_hashes\"][int(\n",
    "            len(self.hash_lists[\"innocent_hashes\"]) * constants.TRAIN_VAL_SPLIT):]\n",
    "\n",
    "        self.dir_lists[\"train_irrelevant_hashes\"] = self.hash_lists[\"irrelevant_hashes\"][:int(\n",
    "            len(self.hash_lists[\"irrelevant_hashes\"]) * constants.TRAIN_VAL_SPLIT)]\n",
    "        self.dir_lists[\"val_irrelevant_hashes\"] = self.hash_lists[\"irrelevant_hashes\"][int(\n",
    "            len(self.hash_lists[\"irrelevant_hashes\"]) * constants.TRAIN_VAL_SPLIT):]\n",
    "# test_infringing_hashes = get_list_directory(class_label = 'infringing', required_image_count = 40,40,200 (im_constants.py), excluded_images = self.hash_lists[\"infringing_hashes\"])\n",
    "        self.dir_lists[\"test_infringing_hashes\"] = self.get_lists_for_test_directory(\n",
    "            \"infringing\", constants.INFRINGING_TEST_IMAGE_COUNT, self.hash_lists[\"infringing_hashes\"])\n",
    "        self.dir_lists[\"test_innocent_hashes\"] = self.get_lists_for_test_directory(\n",
    "            \"innocent\", constants.INNOCENT_TEST_IMAGE_COUNT, self.hash_lists[\"innocent_hashes\"])\n",
    "        self.dir_lists[\"test_irrelevant_hashes\"] = self.get_lists_for_test_directory(\n",
    "            \"irrelevant\", constants.IRRELEVANT_TEST_IMAGE_COUNT, self.hash_lists[\"irrelevant_hashes\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdc65722-acbf-4294-9d3d-7061188396a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_images_to_directories(self):\n",
    "        # running a method - shuffling image lists\n",
    "        self.shuffle_image_lists()\n",
    "        # running a method - split_train_val\n",
    "        self.get_lists_for_each_directory()\n",
    "        # download training_infringing images. arguments - hash_list = train_infrining_hashes, path = train_dir/infringing \n",
    "        ai_functions.parallel_download_images_from_s3(\n",
    "                                self.dir_lists[\"train_infringing_hashes\"],\n",
    "                                (self.tmp_dir + \"/train_dir/infringing\"))\n",
    "        # download validating_infringing images. arguments - hash_list = val_infrining_hashes, path = val_dir/infringing \n",
    "        ai_functions.parallel_download_images_from_s3(\n",
    "                                self.dir_lists[\"val_infringing_hashes\"],\n",
    "                                (self.tmp_dir + \"/val_dir/infringing\"))\n",
    "        # download testing_infringing images. \n",
    "        if len(self.dir_lists[\"test_infringing_hashes\"]) > 0:\n",
    "            ai_functions.parallel_download_images_from_s3(\n",
    "                                    self.dir_lists[\"test_infringing_hashes\"],\n",
    "                                    (self.tmp_dir + \"/test_dir/infringing\"))\n",
    "        # download training_innocent images.\n",
    "        ai_functions.parallel_download_images_from_s3(\n",
    "                                self.dir_lists[\"train_innocent_hashes\"],\n",
    "                                (self.tmp_dir + \"/train_dir/innocent\"))\n",
    "        # download validating_innocent images.\n",
    "        ai_functions.parallel_download_images_from_s3(\n",
    "                                self.dir_lists[\"val_innocent_hashes\"],\n",
    "                                (self.tmp_dir + \"/val_dir/innocent\"))\n",
    "        # download teting_innocent images.\n",
    "        if len(self.dir_lists[\"test_innocent_hashes\"]) > 0:\n",
    "            ai_functions.parallel_download_images_from_s3(\n",
    "                                    self.dir_lists[\"test_innocent_hashes\"],\n",
    "                                    (self.tmp_dir + \"/test_dir/innocent\"))\n",
    "        # download training_irrelevant images.\n",
    "        ai_functions.parallel_download_images_from_s3(\n",
    "                                self.dir_lists[\"train_irrelevant_hashes\"],\n",
    "                                (self.tmp_dir + \"/train_dir/irrelevant\"))\n",
    "        # download validating_irrelevant images.\n",
    "        ai_functions.parallel_download_images_from_s3(\n",
    "                                self.dir_lists[\"val_irrelevant_hashes\"],\n",
    "                                (self.tmp_dir + \"/val_dir/irrelevant\"))\n",
    "        # download testing_irrelevant images.\n",
    "        if len(self.dir_lists[\"test_irrelevant_hashes\"]) > 0:\n",
    "            ai_functions.parallel_download_images_from_s3(\n",
    "                                    self.dir_lists[\"test_irrelevant_hashes\"],\n",
    "                                    (self.tmp_dir + \"/test_dir/irrelevant\"))\n",
    "\n",
    "        self.logger.info(\"Training images added to temporary directories\")\n",
    "        self.items_in_test_infringing = len([name for name in os.listdir(self.tmp_dir+\"/test_dir/infringing/\")])\n",
    "        self.items_in_test_innocent = len([name for name in os.listdir(self.tmp_dir+\"/test_dir/innocent/\")])\n",
    "        self.items_in_test_irrelevant = len([name for name in os.listdir(self.tmp_dir+\"/test_dir/irrelevant/\")])\n",
    "\n",
    "        self.logger.info(\"items in test infringing directory = {}\".format(self.items_in_test_infringing))\n",
    "        self.logger.info(\"items in test innocent directory = {}\".format(self.items_in_test_innocent))\n",
    "        self.logger.info(\"items in test irrelevant directory = {}\".format(self.items_in_test_irrelevant))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ef07dd60-f0b0-48c4-a45c-3622981df720",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_training_directories_for_corrupt_images(self):\n",
    "    directories = [self.tmp_dir + \"/train_dir/infringing\",\n",
    "                   self.tmp_dir + \"/val_dir/infringing\",\n",
    "                   self.tmp_dir + \"/test_dir/infringing\",\n",
    "                   self.tmp_dir + \"/train_dir/innocent\",\n",
    "                   self.tmp_dir + \"/val_dir/innocent\",\n",
    "                   self.tmp_dir + \"/test_dir/innocent\",\n",
    "                   self.tmp_dir + \"/train_dir/irrelevant\",\n",
    "                   self.tmp_dir + \"/val_dir/irrelevant\",\n",
    "                   self.tmp_dir + \"/test_dir/irrelevant\",\n",
    "                   ]\n",
    "    ai_functions.delete_corrupt_images(directories, self.logger)\n",
    "    self.logger.info(\"Training directories checked for corrupt image files.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac29e53d-075b-45c8-bb5c-a0bfd35dc18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VGG(weights, include_top, input_she = (224, 224, 3) for layer in vgg_conv.layers[:]: layer.trainable=False\n",
    "# DENSE_LAYER_SIZE = 1024\n",
    "# DROPOUT_LEVEL = 0.5\n",
    "def build_cnn_model(self):\n",
    "    if self.model_type_info[\"classes\"] == \"two\":\n",
    "        final_layer_size = 2\n",
    "    else:\n",
    "        final_layer_size = 3\n",
    "\n",
    "    vgg_conv = VGG16(\n",
    "        # TODO: Use path once issue is fixed.\n",
    "        # weights = MODEL_WEIGHTS_LOCATION,\n",
    "        weights=\"imagenet\",\n",
    "        include_top=False,\n",
    "        input_shape=(constants.IMAGE_SIZE, constants.IMAGE_SIZE, 3),\n",
    "    )\n",
    "    for layer in vgg_conv.layers[:]:\n",
    "        layer.trainable = False\n",
    "\n",
    "    self.model = Sequential()\n",
    "    self.model.add(vgg_conv)\n",
    "    self.model.add(Flatten())\n",
    "    self.model.add(Dense(constants.DENSE_LAYER_SIZE, activation=\"relu\"))\n",
    "    self.model.add(Dropout(constants.DROPOUT_LEVEL))\n",
    "    self.model.add(Dense(final_layer_size, activation=\"softmax\"))\n",
    "\n",
    "    self.logger.info(\"Neural Network built\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "208de2ae-c660-494e-9164-6273083d4d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageDataGenerator.train_datagenerator.flow_from_directory(data augmentation)\n",
    "# BATCHSIZE = 10\n",
    "def set_generator_directories(self):\n",
    "        self.generators[\"train_generator\"] = self.train_datagen.flow_from_directory(\n",
    "            (self.tmp_dir + \"/train_dir\"),\n",
    "            target_size=(constants.IMAGE_SIZE, constants.IMAGE_SIZE),\n",
    "            batch_size=constants.BATCHSIZE,\n",
    "            class_mode=\"categorical\",\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "        self.generators[\"val_generator\"] = self.val_datagen.flow_from_directory(\n",
    "            (self.tmp_dir + \"/val_dir\"),\n",
    "            target_size=(constants.IMAGE_SIZE, constants.IMAGE_SIZE),\n",
    "            batch_size=constants.BATCHSIZE,\n",
    "            class_mode=\"categorical\",\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        self.generators[\"test_generator\"] = self.test_datagen.flow_from_directory(\n",
    "            (self.tmp_dir + \"/test_dir\"),\n",
    "            target_size=(constants.IMAGE_SIZE, constants.IMAGE_SIZE),\n",
    "            batch_size=constants.BATCHSIZE,\n",
    "            class_mode=\"categorical\",\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "        self.logger.info(\"Data Generators set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "307042b4-b0df-4849-9387-cf0e669fe7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EarlyStopping when validation loss is not reduced\n",
    "# EARLY_STOPPING_PATIENCE = 7\n",
    "# LEARNING_RATE_PATIENCE = 3\n",
    "# LEARNING_RATE_VERBOSE = 1\n",
    "# LEARNING_RATE_REDUCTION_FACTOR = 0.5\n",
    "# LEARNING_RATE_MIN = 0.00001\n",
    "\n",
    "def set_model_training_callbacks(self):\n",
    "        self.model_details[\"early_stopping\"] = EarlyStopping(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=constants.EARLY_STOPPING_PATIENCE,\n",
    "        )\n",
    "\n",
    "        self.model_details[\"learning_rate_reduction\"] = ReduceLROnPlateau(\n",
    "            monitor=\"val_loss\",\n",
    "            patience=constants.LEARNING_RATE_PATIENCE,\n",
    "            verbose=constants.LEARNING_RATE_VERBOSE,\n",
    "            factor=constants.LEARNING_RATE_REDUCTION_FACTOR,\n",
    "            min_lr=constants.LEARNING_RATE_MIN,\n",
    "        )\n",
    "\n",
    "        self.logger.info(\"Model training callbacks set\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b44679ed-5bee-4967-a004-db3d67b539ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(self):\n",
    "        self.model.compile(\n",
    "            loss=\"categorical_crossentropy\",\n",
    "            optimizer=optimizers.RMSprop(),\n",
    "            metrics=[\"acc\"],\n",
    "        )\n",
    "        self.logger.info(\"Model compiled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "70252547-4c06-4228-b65f-61984d67beed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(self):\n",
    "    self.model_details[\"history\"] = self.model.fit(\n",
    "        self.generators[\"train_generator\"],\n",
    "        steps_per_epoch=self.generators[\"train_generator\"].samples / self.generators[\"train_generator\"].batch_size,\n",
    "        epochs=constants.FITTING_EPOCHS,\n",
    "        validation_data=self.generators[\"val_generator\"],\n",
    "        validation_steps=self.generators[\"val_generator\"].samples / self.generators[\"val_generator\"].batch_size,\n",
    "        verbose=constants.FITTING_VERBOSE,\n",
    "        callbacks=[self.model_details[\"learning_rate_reduction\"], self.model_details[\"early_stopping\"]],\n",
    "    )\n",
    "    self.logger.info(\"Model training completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aaea3acf-b188-4b98-92a4-2bd2aae0433a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model_to_temporary_directory(self):\n",
    "    save_model(\n",
    "        model = self.model,\n",
    "        filepath = \"{}/keras_model\".format(self.tmp_dir),\n",
    "        )\n",
    "    self.logger.info(\"Trained model saved to temporary directory\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "83b07151-bb27-4163-9757-36c98b236629",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_trained_model_to_s3(self):\n",
    "    save_keras_model_to_s3(s3_client, self.product_id, self.tmp_dir)\n",
    "    self.logger.info(\"Trained model saved to S3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91058aee-f659-402f-a8bf-e8e3cb3ebf36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_test_probabilities(self):\n",
    "    self.test_probabilities = self.model.predict(\n",
    "        x=self.generators[\"test_generator\"],\n",
    "        verbose=1)\n",
    "    self.test_probabilities_df = pd.DataFrame(self.test_probabilities, columns = np.unique(self.test_data[\"class\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d1b1d2bd-7ff3-43e6-9e42-9472975bdbf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# INNOCENT_SCORE_FACTOR = 0.5\n",
    "def calculate_image_test_score(self, row):\n",
    "    score = 0\n",
    "    score = score + row['infringing']\n",
    "    if self.model_type_info[\"classes\"] == \"three\":\n",
    "        score = score + (row['innocent'] * INNOCENT_SCORE_FACTOR)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f18d3ab-852d-4018-b133-c6e21ddb0b81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "default:Python",
   "language": "python",
   "name": "conda-env-default-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
